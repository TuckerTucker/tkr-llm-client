# LLM Server Configuration
# Enable/disable local LLM integration
LLM_ENABLED=true

# Server port (default: 42002, following tkr-context-kit port scheme)
LLM_PORT=42002

# Path to Python interpreter (use venv with all dependencies)
LLM_PYTHON_PATH=./.venv/bin/python

# Path to the LLM server script (relative to project root)
# Note: LLM server is now in @tkr/llm-client library package
# PYTHONPATH is automatically set to packages/llm-client/ by LLMServerManager
LLM_SERVER_SCRIPT=./packages/llm-client/llm_server/server.py

# Health check timeout in milliseconds (default: 300000 = 5 minutes)
# Set high to allow for initial model download (~11GB for gpt-oss-20b)
LLM_HEALTH_CHECK_TIMEOUT=120000

# Health check polling interval in milliseconds (default: 1000 = 1 second)
LLM_HEALTH_CHECK_INTERVAL=1000

# Auto-restart LLM server if it crashes (default: false)
LLM_AUTO_RESTART=false

# Logging level for LLM server (debug | info | warn | error)
LLM_LOG_LEVEL=info

# Server host binding (default: 127.0.0.1 for localhost only)
LLM_HOST=127.0.0.1

# Number of uvicorn workers (default: 1, increase for concurrent requests)
LLM_WORKERS=1

# GPT-OSS Model Configuration
# Path to local models directory (bypasses default ~/.cache/huggingface)
# All Hugging Face models will download to this location
LLM_MODEL_PATH=./models

# Hugging Face cache directory (set to models dir to avoid default cache)
HF_HOME=./models

# Model name to load (gpt-oss-20b)
# gpt-oss-20b: Harmony format support, uses mlx-community/gpt-oss-20b-MXFP4-Q8
# MXFP4-Q8 is mixed precision quantization (4-bit weights, 8-bit activations)
# Requires ~12GB model memory (16GB+ RAM systems recommended)
# Note: Can use short name or full HuggingFace path
LLM_MODEL_NAME=gpt-oss-20b

# Device to use for inference (auto | mps | cpu)
# 'auto' will use MPS on Apple Silicon, CPU otherwise
LLM_DEVICE=auto

# Quantization mode (int4 | int8 | fp16 | none)
# Note: Ignored for pre-quantized MLX models (gpt-oss-20b uses MXFP4-Q8 quantization)
# This setting only applies when loading unquantized models
LLM_QUANTIZATION=int8

# Claude Code SDK Configuration
# SDK sends requests directly to local LLM server (Anthropic-compatible API)
ANTHROPIC_BASE_URL=http://127.0.0.1:42002

# Fallback Configuration
# API key for Anthropic Claude (fallback when local LLM unavailable)
ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Enable fallback to cloud providers if local LLM fails
ENABLE_CLOUD_FALLBACK=false

# LLM Inference Settings
# Disable streaming for better stability with Harmony format parsing
STREAMING=false
