{
  "name": "@tkr/llm-client",
  "version": "0.1.0",
  "description": "Generic LLM client library with Claude SDK and local LLM server support",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/TuckerTucker/tkr-llm-client.git"
  },
  "bugs": {
    "url": "https://github.com/TuckerTucker/tkr-llm-client/issues"
  },
  "homepage": "https://github.com/TuckerTucker/tkr-llm-client#readme",
  "scripts": {
    "build": "tsc",
    "clean": "rm -rf dist",
    "rebuild": "npm run clean && npm run build",
    "verify-exports": "npx tsx scripts/verify-exports.ts",
    "test": "vitest run",
    "test:watch": "vitest",
    "test:ui": "vitest --ui",
    "test:coverage": "vitest run --coverage",
    "test:live": "vitest run tests/live",
    "server": "./start-llm-server.sh",
    "server:dev": "cd llm_server && uvicorn llm_server.server:app --reload --log-level debug",
    "example:quick": "npx tsx examples/quick-test.ts",
    "example:local": "npx tsx examples/local-inference.ts",
    "example:managed": "npx tsx examples/managed-server.ts",
    "prepublishOnly": "npm run rebuild && npm run verify-exports"
  },
  "keywords": [
    "llm",
    "claude",
    "ai",
    "machine-learning",
    "mlx",
    "local-llm",
    "typescript",
    "fastapi",
    "anthropic"
  ],
  "author": "Tucker",
  "license": "Apache-2.0",
  "dependencies": {
    "@anthropic-ai/claude-agent-sdk": "^0.1.36"
  },
  "peerDependencies": {
    "mlx-lm": "^0.8.0"
  },
  "peerDependenciesMeta": {
    "mlx-lm": {
      "optional": true
    }
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "@vitest/coverage-v8": "^4.0.8",
    "@vitest/ui": "^4.0.8",
    "typescript": "^5.3.0",
    "vitest": "^4.0.8"
  },
  "files": [
    "dist",
    "llm_server",
    "README.md"
  ],
  "engines": {
    "node": ">=18.0.0"
  }
}
